{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811cb835",
   "metadata": {},
   "source": [
    "Definition of Prediction Interval\n",
    "A prediction interval is a statistical concept that provides an estimated range within which a future observation or measurement is expected to fall, with a specified level of confidence.\n",
    "\n",
    "Mathematically, for a random variable Y and a confidence level 1-α, a prediction interval [L, U] satisfies:\n",
    "\n",
    "P(L ≤ Y ≤ U) = 1 - α\n",
    "\n",
    "Where:\n",
    "\n",
    "L is the lower bound of the interval U is the upper bound of the interval\n",
    "Y is the future observation\n",
    "1-α is the confidence level (e.g. 95% for α = 0.05)\n",
    "Mathematical Formulation of Prediction Interval\n",
    "Mathematically speaking, a prediction interval for a future observation Yf derived from an n-observation sample can be stated as:\n",
    "\n",
    "Yf  ± ( tα/2, n-1 ) × s × √(1 + 1/n).\n",
    "\n",
    "where:\n",
    "\n",
    "The projected value is Yf.\n",
    "The t-value for the intended confidence level ( α) and degrees of freedom ( n-1) is tα/2,\n",
    "Standard error of the estimate is s\n",
    "sample size is n.\n",
    "\n",
    "Definition of Confidence Interval\n",
    "A confidence interval in statistics is a range of values that is likely to contain an unknown population parameter with a specified level of confidence. It is constructed around a point estimate and provides a measure of uncertainty.\n",
    "\n",
    "Mathematically, for a population parameter θ and a confidence level (1-α), where α is the significance level, the confidence interval can be expressed as:\n",
    "\n",
    "[θ̂ - z(α/2) × SE(θ̂), θ̂ + z(α/2) × SE(θ̂)]\n",
    "\n",
    "Where:\n",
    "\n",
    "θ̂ (theta hat) is the point estimate of the parameter\n",
    "z(α/2) is the critical value from the standard normal distribution\n",
    "SE(θ̂) is the standard error of the estimate\n",
    "Mathematical Formulation of Confidence Interval\n",
    "The general form of a confidence interval (CI) is:\n",
    "\n",
    "Point Estimate ± (Critical Value × Standard Error)\n",
    "\n",
    "For Population Mean (Known Population Standard Deviation)\n",
    "\n",
    "When the population standard deviation (σ) is known:\n",
    "\n",
    "CI = x̄ ± (z × σ/√n)\n",
    "\n",
    "Where:\n",
    "\n",
    "x̄ is the sample mean\n",
    "z is the z-score from the standard normal distribution\n",
    "σ is the known population standard deviation\n",
    "n is the sample size\n",
    "For Population Mean (Unknown Population Standard Deviation)\n",
    "\n",
    "When the population standard deviation is unknown:\n",
    "\n",
    "CI = x̄ ± (t × s/√n)\n",
    "\n",
    "Where:\n",
    "\n",
    "x̄ is the sample mean\n",
    "t is the t-score from the t-distribution\n",
    "s is the sample standard deviation\n",
    "n is the sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a96394",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a93dace7",
   "metadata": {},
   "source": [
    "Ensemble learning is a method where we use many small models instead of just one. Each of these models may not be very strong on its own, but when we put their results together, we get a better and more accurate answer. It's like asking a group of people for advice instead of just one person—each one might be a little wrong, but together, they usually give a better answer.\n",
    "\n",
    "There are three main types of ensemble methods:\n",
    "\n",
    "Bagging (Bootstrap Aggregating):\n",
    "\n",
    "Models are trained independently on different random subsets of the training data. Their results are then combined—usually by averaging (for regression) or voting (for classification). This helps reduce variance and prevents overfitting.\n",
    "\n",
    "Boosting:\n",
    "\n",
    "Models are trained one after another. Each new model focuses on fixing the errors made by the previous ones. The final prediction is a weighted combination of all models, which helps reduce bias and improve accuracy.\n",
    "\n",
    "Stacking (Stacked Generalization):   \n",
    "\n",
    "Multiple different models (often of different types) are trained, and their predictions are used as inputs to a final model, called a meta-model. The meta-model learns how to best combine the predictions of the base models, aiming for better performance than any individual model.\n",
    "\n",
    "1. Bagging Algorithm\n",
    "Bagging classifier can be used for both regression and classification tasks. Here is an overview of Bagging classifier algorithm:\n",
    "\n",
    "Bootstrap Sampling: Divides the original training data into ‘N’ subsets and randomly selects a subset with replacement in some rows from other subsets. This step ensures that the base models are trained on diverse subsets of the data and there is no class imbalance.\n",
    "Base Model Training: For each bootstrapped sample we train a base model independently on that subset of data. These weak models are trained in parallel to increase computational efficiency and reduce time consumption. We can use different base learners i.e. different ML models as base learners to bring variety and robustness.\n",
    "Prediction Aggregation: To make a prediction on testing data combine the predictions of all base models. For classification tasks it can include majority voting or weighted majority while for regression it involves averaging the predictions.\n",
    "Out-of-Bag (OOB) Evaluation: Some samples are excluded from the training subset of particular base models during the bootstrapping method. These “out-of-bag” samples can be used to estimate the model’s performance without the need for cross-validation.\n",
    "Final Prediction: After aggregating the predictions from all the base models, Bagging produces a final prediction for each instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2bab7",
   "metadata": {},
   "source": [
    "2. Boosting Algorithm\n",
    "Boosting is an ensemble technique that combines multiple weak learners to create a strong learner. Weak models are trained in series such that each next model tries to correct errors of the previous model until the entire training dataset is predicted correctly. One of the most well-known boosting algorithms is AdaBoost (Adaptive Boosting). Here is an overview of Boosting algorithm:\n",
    "\n",
    "Initialize Model Weights: Begin with a single weak learner and assign equal weights to all training examples.\n",
    "Train Weak Learner: Train weak learners on these dataset.\n",
    "Sequential Learning: Boosting works by training models sequentially where each model focuses on correcting the errors of its predecessor. Boosting typically uses a single type of weak learner like decision trees.\n",
    "Weight Adjustment: Boosting assigns weights to training datapoints. Misclassified examples receive higher weights in the next iteration so that next models pay more attention to them."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
