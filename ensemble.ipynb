{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3226c7",
   "metadata": {},
   "source": [
    "Ensemble learning is a method where we use many small models instead of just one. Each of these models may not be very strong on its own, but when we put their results together, we get a better and more accurate answer. It's like asking a group of people for advice instead of just one person—each one might be a little wrong, but together, they usually give a better answer.\n",
    "\n",
    "There are three main types of ensemble methods:\n",
    "\n",
    "Bagging (Bootstrap Aggregating):\n",
    "\n",
    "Models are trained independently on different random subsets of the training data. Their results are then combined—usually by averaging (for regression) or voting (for classification). This helps reduce variance and prevents overfitting.\n",
    "\n",
    "Boosting:\n",
    "\n",
    "Models are trained one after another. Each new model focuses on fixing the errors made by the previous ones. The final prediction is a weighted combination of all models, which helps reduce bias and improve accuracy.\n",
    "\n",
    "Stacking (Stacked Generalization):   \n",
    "\n",
    "Multiple different models (often of different types) are trained, and their predictions are used as inputs to a final model, called a meta-model. The meta-model learns how to best combine the predictions of the base models, aiming for better performance than any individual model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8129f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2916172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\KIIT\\Downloads\\cleaned_titanic_data.csv\")\n",
    "X = df.drop(columns = 'Pclass', axis = 1)\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c4f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting test and train data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c20687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Classifier\n",
    "\n",
    "base_classifier = DecisionTreeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b7ce3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Bagging \n",
    "\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14352f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n",
      "Random Subspace Method Accuracy: 0.9832402234636871\n"
     ]
    }
   ],
   "source": [
    "# Random Forest - Bagging\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Random Subspace Method - Bagging\n",
    "# This is similar to RandomForest but with a single tree and max_features < total features\n",
    "subspace_classifier = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    max_features=2,  \n",
    "    random_state=42\n",
    ")\n",
    "subspace_classifier.fit(X_train, y_train)\n",
    "subspace_pred = subspace_classifier.predict(X_test)\n",
    "subspace_accuracy = accuracy_score(y_test, subspace_pred)\n",
    "print(\"Random Subspace Method Accuracy:\", subspace_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb5db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Boosting\n",
    "\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2643f8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Machines Accuracy: 1.0\n",
      "XGBoost Accuracy: 1.0\n",
      "AdaBoost Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [20:47:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machines\n",
    "\n",
    "gbm_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "gbm_pred = gbm_classifier.predict(X_test)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_pred)\n",
    "print(\"Gradient Boosting Machines Accuracy:\", gbm_accuracy)\n",
    "\n",
    "# Extreme Gradient Boosting Machines (XGBoost)\n",
    "\n",
    "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "xgb_pred = xgb_classifier.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "\n",
    "# AdaBoost (Adaptive Boosting)\n",
    "# Already done in previous cell, but for completeness:\n",
    "adaboost_pred = adaboost_classifier.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test, adaboost_pred)\n",
    "print(\"AdaBoost Accuracy:\", adaboost_accuracy)\n",
    "\n",
    "# CatBoost\n",
    "\n",
    "catboost_classifier = CatBoostClassifier(iterations=100, learning_rate=0.1, verbose=0, random_seed=42)\n",
    "catboost_classifier.fit(X_train, y_train)\n",
    "catboost_pred = catboost_classifier.predict(X_test)\n",
    "catboost_accuracy = accuracy_score(y_test, catboost_pred)\n",
    "print(\"CatBoost Accuracy:\", catboost_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9087a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
